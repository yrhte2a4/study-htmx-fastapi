import asyncio
import json
import streamlit as st
from agents import Agent, Runner, set_tracing_disabled, AsyncOpenAI
from agents.mcp import MCPServerStdio
from agents.models.openai_chatcompletions import OpenAIChatCompletionsModel
import os
from dotenv import load_dotenv

# åˆæœŸè¨­å®š
load_dotenv()
set_tracing_disabled(disabled=True)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
MCP_SERVER_PACKAGE = os.getenv("MCP_SERVER_PACKAGE", "awslabs.aws-documentation-mcp-server@latest")

# UIè¨­å®š
st.title("OpenAI Agent SDK MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
st.text("Azure OpenAIã¨MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

with st.expander("è¨­å®šçŠ¶æ³"):
    st.write(f"**Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**: {AZURE_OPENAI_ENDPOINT or 'æœªè¨­å®š'}")
    st.write(f"**ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå**: {AZURE_OPENAI_DEPLOYMENT_NAME or 'æœªè¨­å®š'}")
    st.write(f"**APIãƒãƒ¼ã‚¸ãƒ§ãƒ³**: {AZURE_OPENAI_API_VERSION}")
    st.write(f"**MCPã‚µãƒ¼ãƒãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: {MCP_SERVER_PACKAGE}")

question = st.text_input("è³ªå•ã‚’å…¥åŠ›", "Bedrock AgentCoreã§ã¯ã©ã‚“ãªã“ã¨ãŒã§ãã‚‹ï¼ŸAWSãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦æ•™ãˆã¦")

def create_azure_openai_client():
    """Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    return AsyncOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        base_url=f"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT_NAME}",
        default_headers={"api-key": AZURE_OPENAI_API_KEY},
        default_query={"api-version": AZURE_OPENAI_API_VERSION},
    )

def create_mcp_server():
    """MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½œæˆ"""
    return MCPServerStdio(params={
        "command": "uvx",
        "args": [MCP_SERVER_PACKAGE]
    })

def create_agent(mcp_server, custom_client):
    """Azure OpenAIã‚’ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
    return Agent(
        name="Assistant",
        instructions="""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ãªå›žç­”ã‚’æä¾›ã—ã¾ã™ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦æœ€æ–°ã‹ã¤æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ä»¥ä¸‹ã®å ´åˆã¯ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„ï¼š
- æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªå ´åˆ
- è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã‚„å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‚ç…§ãŒå¿…è¦ãªå ´åˆ
- å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚„çµ±è¨ˆæƒ…å ±ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆ

ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹æƒ…å ±ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†é©åˆ‡ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚""",
        model=OpenAIChatCompletionsModel(
            model=AZURE_OPENAI_DEPLOYMENT_NAME,
            openai_client=custom_client,
        ),
        mcp_servers=[mcp_server],
    )

def extract_tool_executions(result):
    """çµæžœã‹ã‚‰ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œæƒ…å ±ã‚’æŠ½å‡º"""
    tool_executions = []
    
    if not hasattr(result, 'raw_responses'):
        return tool_executions
    
    for raw_response in result.raw_responses:
        if not hasattr(raw_response, 'output'):
            continue
            
        for output_item in raw_response.output:
            if hasattr(output_item, 'name') and hasattr(output_item, 'arguments'):
                try:
                    args_dict = json.loads(output_item.arguments)
                except json.JSONDecodeError:
                    args_dict = output_item.arguments
                
                tool_executions.append({
                    'name': output_item.name,
                    'arguments': args_dict,
                    'call_id': getattr(output_item, 'call_id', 'unknown')
                })
    
    return tool_executions

def display_tool_executions(tool_executions):
    """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå±¥æ­´ã‚’è¡¨ç¤º"""
    if not tool_executions:
        st.info("ã“ã®è³ªå•ã§ã¯ãƒ„ãƒ¼ãƒ«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    with st.expander(f"ðŸ”§ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå±¥æ­´ ({len(tool_executions)}å›ž)"):
        for i, tool in enumerate(tool_executions, 1):
            st.markdown(f"### `{tool['name']}`")
            
            if isinstance(tool['arguments'], dict):
                st.markdown("**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**")
                for key, value in tool['arguments'].items():
                    if isinstance(value, str) and len(value) > 100:
                        with st.expander(f"ðŸ“„ {key}"):
                            st.text(value)
                    elif isinstance(value, (list, dict)):
                        st.code(f"{key}: {json.dumps(value, indent=2, ensure_ascii=False)}", language="json")
                    else:
                        st.code(f"{key}: {value}")
            else:
                st.code(tool['arguments'], language="json")
            
            st.caption(f"Call ID: {tool['call_id']}")
            
            if i < len(tool_executions):
                st.divider()

def display_usage_info(result):
    """Usageæƒ…å ±ã‚’è¡¨ç¤º"""
    usage_info = None
    if hasattr(result, 'raw_responses'):
        for raw_response in result.raw_responses:
            if hasattr(raw_response, 'usage'):
                usage_info = raw_response.usage
                break
    
    if not usage_info:
        return
    
    with st.expander("ðŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡"):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", usage_info.input_tokens)
        with col2:
            st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", usage_info.output_tokens)
        with col3:
            st.metric("åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³", usage_info.total_tokens)
        
        if (hasattr(usage_info, 'input_tokens_details') and 
            usage_info.input_tokens_details and
            hasattr(usage_info.input_tokens_details, 'cached_tokens')):
            st.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³: {usage_info.input_tokens_details.cached_tokens}")

def display_available_tools(tools):
    """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
    if not tools:
        return
        
    with st.expander(f"ðŸ”§ åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ« ({len(tools)}å€‹)"):
        for i, tool in enumerate(tools, 1):
            st.markdown(f"### {i}. ðŸ› ï¸ {tool.name}")
            st.markdown(f"ðŸ“ {tool.description}")
            if i < len(tools):
                st.markdown("---")

async def run_agent_async(question, container):
    """éžåŒæœŸã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
    mcp_server = create_mcp_server()
    custom_client = create_azure_openai_client()
    
    async with mcp_server:
        # ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        tools = await mcp_server.list_tools()
        display_available_tools(tools)
        
        agent = create_agent(mcp_server, custom_client)
        result = await Runner.run(agent, question)
        
        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå±¥æ­´ã‚’è¡¨ç¤º
        tool_executions = extract_tool_executions(result)
        display_tool_executions(tool_executions)
        
        # æœ€çµ‚å›žç­”ã‚’è¡¨ç¤º
        with container:
            if hasattr(result, 'final_output'):
                st.markdown(result.final_output)
            else:
                st.markdown(str(result))
            
            display_usage_info(result)

def check_configuration():
    """å¿…è¦ãªè¨­å®šãŒã™ã¹ã¦æƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    missing = []
    if not AZURE_OPENAI_ENDPOINT:
        missing.append("AZURE_OPENAI_ENDPOINT")
    if not AZURE_OPENAI_API_KEY:
        missing.append("AZURE_OPENAI_API_KEY")
    if not AZURE_OPENAI_DEPLOYMENT_NAME:
        missing.append("AZURE_OPENAI_DEPLOYMENT_NAME")
    
    return missing

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if st.button("è³ªå•ã™ã‚‹"):
    missing_config = check_configuration()
    
    if missing_config:
        st.error(f"ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_config)}")
        st.info("`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªè¨­å®šã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å›žç­”ã‚’ç”Ÿæˆä¸­â€¦"):
            container = st.container()
            
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(run_agent_async(question, container))
            except Exception as e:
                error_message = str(e)
                if "RateLimitReached" in error_message:
                    st.error("ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚60ç§’å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                    st.info("ðŸ’¡ **å¯¾å‡¦æ–¹æ³•**:\n- 60ç§’å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œ\n- Azure OpenAIãƒãƒ¼ã‚¿ãƒ«ã§ã‚¯ã‚©ãƒ¼ã‚¿å¢—åŠ ã‚’ç”³è«‹\n- ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´")
                else:
                    st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {error_message}")
            finally:
                loop.close()