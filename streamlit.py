import asyncio
import streamlit as st
from datetime import datetime
from agents import Agent, Runner, set_tracing_disabled, AsyncOpenAI
from agents.mcp import MCPServerStdio
from agents.models.openai_chatcompletions import OpenAIChatCompletionsModel
import os
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒˆãƒ¬ãƒ¼ã‚¹æ©Ÿèƒ½ã‚’ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆã‚ªãƒ•ã«ã—ãªã„ã¨401ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ï¼‰
set_tracing_disabled(disabled=True)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
MCP_SERVER_PACKAGE = os.getenv("MCP_SERVER_PACKAGE", "awslabs.aws-documentation-mcp-server@latest")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
st.title("OpenAI Agent SDK MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
st.text("Azure OpenAIã¨MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

# è¨­å®šçŠ¶æ³ã‚’è¡¨ç¤º
with st.expander("è¨­å®šçŠ¶æ³"):
    st.write(f"**Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**: {AZURE_OPENAI_ENDPOINT or 'æœªè¨­å®š'}")
    st.write(f"**ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå**: {AZURE_OPENAI_DEPLOYMENT_NAME or 'æœªè¨­å®š'}")
    st.write(f"**APIãƒãƒ¼ã‚¸ãƒ§ãƒ³**: {AZURE_OPENAI_API_VERSION}")
    st.write(f"**MCPã‚µãƒ¼ãƒãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: {MCP_SERVER_PACKAGE}")

question = st.text_input("è³ªå•ã‚’å…¥åŠ›", "Bedrock AgentCoreã§ã¯ã©ã‚“ãªã“ã¨ãŒã§ãã‚‹ï¼ŸAWSãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦æ•™ãˆã¦")

def create_azure_openai_client():
    """Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    return AsyncOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        base_url=f"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT_NAME}",
        default_headers={"api-key": AZURE_OPENAI_API_KEY},
        default_query={"api-version": AZURE_OPENAI_API_VERSION},
    )

def create_mcp_server():
    """MCPã‚µãƒ¼ãƒãƒ¼ã‚’ä½œæˆ"""
    return MCPServerStdio(params={
        "command": "uvx",
        "args": [MCP_SERVER_PACKAGE]
    })

def create_agent(mcp_server, custom_client):
    """Azure OpenAIã‚’ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
    return Agent(
        name="Assistant",
        instructions="""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦æœ€æ–°ã‹ã¤æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ä»¥ä¸‹ã®å ´åˆã¯ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ï¼š
- æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªå ´åˆ
- è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã‚„å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‚ç…§ãŒå¿…è¦ãªå ´åˆ
- å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã‚„çµ±è¨ˆæƒ…å ±ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆ

ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹æƒ…å ±ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†é©åˆ‡ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚""",
        model=OpenAIChatCompletionsModel(
            model=AZURE_OPENAI_DEPLOYMENT_NAME,
            openai_client=custom_client,
        ),
        mcp_servers=[mcp_server],
    )

async def stream_response(agent, question, container):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º"""
    text_holder = container.empty()
    
    try:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
        result = await Runner.run(agent, question)
        
        # çµæœã‚’è¡¨ç¤º
        if hasattr(result, 'final_output'):
            text_holder.markdown(result.final_output)
        else:
            text_holder.markdown(str(result))
        
        # Usageæƒ…å ±ã‚’è¡¨ç¤º
        usage_info = None
        if hasattr(result, 'raw_responses'):
            for raw_response in result.raw_responses:
                if hasattr(raw_response, 'usage'):
                    usage_info = raw_response.usage
                    break
        
        if usage_info:
            with st.expander("ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡"):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", usage_info.input_tokens)
                with col2:
                    st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", usage_info.output_tokens)
                with col3:
                    st.metric("åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³", usage_info.total_tokens)
                
                if hasattr(usage_info, 'input_tokens_details') and usage_info.input_tokens_details:
                    if hasattr(usage_info.input_tokens_details, 'cached_tokens'):
                        st.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³: {usage_info.input_tokens_details.cached_tokens}")
        
        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œæƒ…å ±ã‚’æŠ½å‡ºãƒ»è¡¨ç¤ºï¼ˆVSCode IDEé¢¨ï¼‰
        tool_executions = []
        
        # raw_responsesã‹ã‚‰ResponseFunctionToolCallã‚’æŠ½å‡º
        if hasattr(result, 'raw_responses'):
            for raw_response in result.raw_responses:
                if hasattr(raw_response, 'output'):
                    for output_item in raw_response.output:
                        # ResponseFunctionToolCallã‚’æ¤œå‡º
                        if hasattr(output_item, 'name') and hasattr(output_item, 'arguments'):
                            try:
                                import json
                                args_dict = json.loads(output_item.arguments)
                                tool_executions.append({
                                    'name': output_item.name,
                                    'arguments': args_dict,
                                    'call_id': getattr(output_item, 'call_id', 'unknown')
                                })
                            except json.JSONDecodeError:
                                tool_executions.append({
                                    'name': output_item.name,
                                    'arguments': output_item.arguments,
                                    'call_id': getattr(output_item, 'call_id', 'unknown')
                                })
        
        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œæƒ…å ±ã‚’è¡¨ç¤ºï¼ˆVSCode IDEé¢¨ï¼‰
        if tool_executions:
            with st.expander(f"ğŸ”§ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ ({len(tool_executions)}å›)"):
                for i, tool in enumerate(tool_executions, 1):
                    # ãƒ„ãƒ¼ãƒ«åã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¡¨ç¤º
                    st.markdown(f"### ğŸ› ï¸ `{tool['name']}`")
                    
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ•´å½¢è¡¨ç¤º
                    if isinstance(tool['arguments'], dict):
                        st.markdown("**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**")
                        for key, value in tool['arguments'].items():
                            # å€¤ã®å‹ã«å¿œã˜ã¦è¡¨ç¤ºã‚’èª¿æ•´
                            if isinstance(value, str) and len(value) > 100:
                                # é•·ã„æ–‡å­—åˆ—ã¯æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤º
                                with st.expander(f"ğŸ“„ {key}"):
                                    st.text(value)
                            elif isinstance(value, (list, dict)):
                                # ãƒªã‚¹ãƒˆã‚„è¾æ›¸ã¯JSONè¡¨ç¤º
                                st.code(f"{key}: {json.dumps(value, indent=2, ensure_ascii=False)}", language="json")
                            else:
                                # ãã®ä»–ã¯é€šå¸¸è¡¨ç¤º
                                st.code(f"{key}: {value}")
                    else:
                        # è¾æ›¸ã§ãªã„å ´åˆã¯ãã®ã¾ã¾è¡¨ç¤º
                        st.code(tool['arguments'], language="json")
                    
                    # Call IDã‚’å°ã•ãè¡¨ç¤º
                    st.caption(f"Call ID: {tool['call_id']}")
                    
                    # åŒºåˆ‡ã‚Šç·šï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
                    if i < len(tool_executions):
                        st.divider()
        else:
            st.info("ğŸ¤– ã“ã®è³ªå•ã§ã¯ãƒ„ãƒ¼ãƒ«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        error_message = str(e)
        if "RateLimitReached" in error_message:
            text_holder.error("âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚60ç§’å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            st.info("ğŸ’¡ **å¯¾å‡¦æ–¹æ³•**:\n- 60ç§’å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œ\n- Azure OpenAIãƒãƒ¼ã‚¿ãƒ«ã§ã‚¯ã‚©ãƒ¼ã‚¿å¢—åŠ ã‚’ç”³è«‹\n- ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´")
        else:
            text_holder.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")

async def run_agent_async(question, container):
    """éåŒæœŸã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
    mcp_server = create_mcp_server()
    custom_client = create_azure_openai_client()
    
    async with mcp_server:
        # ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        tools = await mcp_server.list_tools()
        if tools:
            with st.expander(f"åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ« ({len(tools)}å€‹)"):
                for tool in tools:
                    st.write(f"- **{tool.name}**: {tool.description}")
        
        agent = create_agent(mcp_server, custom_client)
        await stream_response(agent, question, container)

# è¨­å®šãƒã‚§ãƒƒã‚¯
def check_configuration():
    """å¿…è¦ãªè¨­å®šãŒã™ã¹ã¦æƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    missing = []
    if not AZURE_OPENAI_ENDPOINT:
        missing.append("AZURE_OPENAI_ENDPOINT")
    if not AZURE_OPENAI_API_KEY:
        missing.append("AZURE_OPENAI_API_KEY")
    if not AZURE_OPENAI_DEPLOYMENT_NAME:
        missing.append("AZURE_OPENAI_DEPLOYMENT_NAME")
    
    return missing

# ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰ç”Ÿæˆé–‹å§‹
if st.button("è³ªå•ã™ã‚‹"):
    missing_config = check_configuration()
    
    if missing_config:
        st.error(f"ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_config)}")
        st.info("`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªè¨­å®šã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­â€¦"):
            container = st.container()
            
            # éåŒæœŸå®Ÿè¡Œ
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(
                    run_agent_async(question, container)
                )
            except Exception as e:
                st.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            finally:
                loop.close()